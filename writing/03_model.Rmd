---
output:
  bookdown::pdf_document2

---

We reasoned that, habituation, dishabituation and complexity effects are consequences of encoding. For example, habituation occurs when each repetition of a stimulus refines the representation of a concept until encoding is complete. Dishabituation then occurs when a stimulus deviates from the concept learned during habituation. And complex stimuli might be harder to encode, which slows down encoding. We therefore formalized the learning problem that participants face in a simple habituation experiment as a form of Bayesian concept learning [@goodman2008rational;@tenenbaum1999bayesian]. In this setting, learners are tasked with learning a concept by observing samples from it, and each concept has a set of features that can be either on or off, to represent complexity the number of features being on. 

## Learning 

RANCH's goal is to learn a concept $\theta$, which is a set of probabilities for independent binary features $\theta_{1,2,..,n}$, where n is the number of features. $\theta$ in turn generates exemplars $y$: instantiations of $\bar{\theta}$, where each feature $y_{1,2,..,n}$ is either on or off. Each feature $\theta_i$ and its corresponding exemplar $y_i$ form a Beta-Bernoulli process:

\begin{eqnarray}
p(\theta_i) \sim Beta(\alpha_i,\beta_i) \\
p(y_i|\theta_i) \sim Bernoulli(\theta_i)
\end{eqnarray}

Since the features are independent, this relationship holds for the entire concept $\theta$. 

This formulation mirrors the models proposed previously to account for infant looking behavior, but it assumes that stimuli are encoded perfectly and instantaneously[@kidd2012goldilocks; @poli2020infants]. However, to model the precise time course of attention, we instead suggest that participants gather repeated noisy samples $\bar{z}$ from the exemplars, instead of directly observing them. For any sample $z$ from an exemplar $y$ there is a small probability $\epsilon$ to misperceive the feature as off when it was actually on, and vice versa.

Therefore, by making noisy observations $\bar{z}$, the learner obtains information about the true identity of the exemplar $y$, and by extension, about the concept $\theta$. By Bayes’ rule:
\begin{eqnarray}
P(\theta|\bar{z}) &= p(\bar{z}|y) p(y|\theta) p(\theta) / p(\bar{z})
\end{eqnarray}
where $p(\bar{z}|y_i)$ is fully described by $\epsilon$, and $p(y|\theta)$ by Bernoulli processes as in Eq. 2. Figure 1 shows the plate diagram illustrating the RANCH's model architecture. 

```{r plate_diagram, echo = FALSE, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=2, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "Graphical representation of RANCH. Circles indicate random variables. The square indicates fixed model parameters."}
img <- png::readPNG("figs/plate_diagram.png")
grid::grid.raster(img)
```

## Sampling

The formulation of the model as taking noisy samples from exemplars allows us to do two things: First, we can explicitly model the learner's on-the-fly decisions by asking the model to decide, after every sample $z$, whether it wants to continue sampling from the same stimulus or not. This is in contrast to previous work which describes how entire stimuli affect an ideal learner model [@kidd2012goldilocks; @poli2020infants]. In these cases, we can only link information-theoretic measures to looking data, but not provide a mechanism for how these measures could control moment-to-moment sampling decisions. Second, a consequence of making a decision at every time step is that we can study the behavior of another information-theoretic measure: the expected information gain (EIG). EIG is commonly used in rational analyses of information-seeking behavior - that is to assess whether information-seeking is optimal with respect to the learning task [@markant2012does; @oaksford1994rational]. Importantly, EIG is a forward-looking measure that considers the potential for learning from the next sample. Since previous models operate on the level of a whole stimulus, rather than a series of incomplete, noisy individual samples, EIG would look forward to the next stimulus in these models, rather than to the next sample, and therefore not capture the in-the-moment decision of whether to keep looking. Relating EIG to looking time is therefore only possible in models with temporally extended perception. 

RANCH computes EIG by weighing the information gain from each possible next observation by the probability of that observation. We defined information gain as the KL-divergence between the hypothetical posterior after observing a future sample $z_{t+1}$ and the current posterior:
\begin{eqnarray}
EIG(z_{t+1}) = \sum_{z_{t+1} \in [0,1]} p(z_{t+1}|\theta_t) * D_{KL}(\theta_{t+1} || p(\theta_t))
\end{eqnarray}
Finally, to get actual sampling behavior from the model, it has to convert EIG into a binary decision about whether to continue looking at the current sample, or to advance to the next trial. The model does so using a Luce choice between the EIG from the next sample and a constant EIG from looking away.
\begin{eqnarray}
p(look) = \frac{EIG(z_{t+1})}{EIG(z_{t+1})+EIG(world)}
\end{eqnarray}


## Experimental Setting 

To model the behavioral experiment, we first represented the stimuli as binary-valued vectors indicating the presence (1) or absence (0) of each feature. All stimuli vectors were chosen to be  length 6 to provide sufficient representational flexibility. Complex stimuli were represented as having three 1s and simple stimuli were represented as having one 1, with the rest of the elements 0s. Individual stimuli are then assembled into sequences to reflect the stimuli sequences in the behavioral experiment. We ran four types of sequences, differing in the position of the deviant: The sequence could either be a pure habituation sequence with six background stimuli, or a deviant deviant appeared at positions 2, 4 or 6. For a particular sequence, we constructed the deviant stimulus based on the background stimulus to make sure that they were always maximally different and had the same number of features present. 

The model then chose how to sample based on the three information-theoretic linking hypotheses (EIG, surprisal and KL), as well as the baseline linking hypotheses (random looking and no noise).

We let the model run 500 times for each sequence to obtain a reasonably precise estimate of the model’s behavior. 



## Parameter estimation


```{r getparams, include=FALSE}


c_model_params <- c_res %>% 
  distinct(im_type, params_info) %>% 
  separate(params_info, into = c("ae", "ae_val", 
                                 "be", "be_val", 
                                 "ap", "ap_val", 
                                 "bp", "bp_val", 
                                 "np", "np_val", 
                                 "wEIG", "wEIG_val"), 
           sep = "_")

```


We performed an iterative grid search in parameter space for each linking hypothesis. We a priori constrained our parameter space on the prior beta distribution to have shape parameters $\alpha_{\theta} > \beta_{\theta}$, which describe the prior beliefs as “more likely to see the absence of a feature than the presence of a feature”.  We then searched for the priors over the concept ($\theta$), the noise parameter that decides how likely a feature would be misperceived ($\epsilon$), and the constant EIG from the world ($EIG(world)$). The prior over the noise parameter was fixed for all searches ($\alpha_{\epsilon}$ = 1;$\beta_{\epsilon}$ = 10). We selected the parameters that achieved the highest correlation with the behavioral data (EIG: $\alpha_{\theta}$ = `r filter(c_model_params, im_type == "EIG")$ap_val`, $\beta_{\theta}$ = `r filter(c_model_params, im_type == "EIG")$bp_val`, $\epsilon$ = `r filter(c_model_params, im_type == "EIG")$np_val`, $EIG(world)$ = `r filter(c_model_params, im_type == "EIG")$wEIG_val`; KL: $\alpha_{\theta}$ = `r filter(c_model_params, im_type == "KL")$ap_val`, $\beta_{\theta}$ = `r filter(c_model_params, im_type == "KL")$bp_val`, $\epsilon$ = `r filter(c_model_params, im_type == "KL")$np_val`, $EIG(world)$ = `r filter(c_model_params, im_type == "KL")$wEIG_val`; Surprisal: $\alpha_{\theta}$ = `r filter(c_model_params, im_type == "surprisal")$ap_val`, $\beta_{\theta}$ = `r filter(c_model_params, im_type == "surprisal")$bp_val`, $\epsilon$ = `r filter(c_model_params, im_type == "surprisal")$np_val`, $EIG(world)$ = `r filter(c_model_params, im_type == "surprisal")$wEIG_val`). 


## Results and Discussion 


```{r experiment_res, echo = FALSE, fig.env = "figure*", fig.pos = "t!", fig.width=6.6, fig.height=5, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Continuous time model using different linking hypotheses provide qualitatively indistinguishable fits to the behavioral data. All model results are log-transformed and adjusted to be at the same scale and intercepts as the log-transformed behavioral data. The solid lines represent human data, and the dotted lines represent the model's results. Red lines indicated results for complex stimuli, and blue lines indicated results for simple stimuli."}

viz_df <- bind_rows(nn_res, r_res, c_res) %>% 
  mutate(res_type = im_type, 
        res_val = scaled_log_sample_n) %>%
  select(res_type, complexity, trial_number, sequence_scheme_print, res_val) %>% 
  bind_rows(
    b_res_print %>% mutate(res_type = "behavioral", res_val = log_trial_looking_time) %>% 
      select(res_type, complexity, trial_number, sequence_scheme_print, res_val)
  ) %>% 
  mutate(res_type_print = case_when(
    res_type == "surprisal" ~ "Surprisal",
    res_type == "EIG_nn" ~ "No Noise", 
    res_type == "random" ~ "Random", 
    res_type == "EIG" ~ "EIG", 
    res_type == "KL" ~ "KL", 
    res_type == "behavioral" ~ "Behavioral"
  ))

library(patchwork)

viz_df$res_type_print <- factor(viz_df$res_type_print, levels = c("Behavioral", "EIG", "KL", "Surprisal", "No Noise", "Random"))
viz_df$sequence_scheme_print <- factor(viz_df$sequence_scheme_print, 
                                       levels = c("No Deviant",  "Deviant at 2nd Trial", 
                                                  "Deviant at 4th Trial", "Deviant at Last Trial"))




behavioral_plot <- viz_df %>% 
  filter(res_type_print == "Behavioral") %>% 
   ggplot(aes(x = trial_number, y = res_val, color = complexity)) +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2),  fatten = 0) + 
   stat_summary(fun.data = "mean_cl_boot",
                geom = "line", position = position_dodge(width = .2)) +
  
  facet_grid(res_type_print~sequence_scheme_print) +
  scale_x_continuous(breaks = seq(1, 6, 1)) + 
  #langcog::theme_mikabr() +
  theme_classic()+
  langcog::scale_color_solarized(name = "Trial Complexity",labels = c("Complex", "Simple")) + 
  theme(strip.background = element_blank(), 
        axis.title.x = element_blank(), 
        axis.title.y = element_blank(),
        axis.line.x = element_blank(), 
        axis.ticks = element_blank(),
        axis.text.x = element_blank(),
        legend.position = "none", 
        panel.border = element_rect(color = "black", fill = NA), 
        
        )

model_plot <- viz_df %>% 
  filter(res_type_print != "Behavioral") %>% 
  ggplot(aes(x = trial_number, y = res_val, color = complexity)) +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2),  fatten = 0) + 
   stat_summary(fun.data = "mean_cl_boot",
                geom = "line", position = position_dodge(width = .2)) +
  
  facet_grid(res_type_print~sequence_scheme_print) +
  scale_x_continuous(breaks = seq(1, 6, 1)) + 
  #langcog::theme_mikabr() +
  theme_classic()+
  langcog::scale_color_solarized(name = "Trial Complexity",labels = c("Complex", "Simple")) + 
  xlab("Trial Number") + 
  theme(strip.background = element_blank(), 
        axis.title.y = element_blank(),
        strip.text.x = element_blank(),
        legend.position = "bottom")


behavioral_plot + model_plot + plot_layout(heights = c(1, 5))
```




```{r xtable, results="asis"}

scaled_eig_res_sum <- summarise_scaled_tidy_sim_res(c_res %>% filter(im_type == "EIG"))
scaled_kl_res_sum <- summarise_scaled_tidy_sim_res(c_res %>% filter(im_type == "KL"))
scaled_s_res_sum <- summarise_scaled_tidy_sim_res(c_res %>% filter(im_type == "surprisal"))
scaled_r_res_sum <- summarise_scaled_tidy_sim_res(r_res)
scaled_nn_res_sum <- summarise_scaled_tidy_sim_res(nn_res)

eig_sim_b_res <- scaled_eig_res_sum %>% 
  left_join(b_res_summary, by = c("trial_number", "sequence_scheme", "sequence_scheme_print", "complexity")) 
kl_sim_b_res <- scaled_kl_res_sum %>% 
  left_join(b_res_summary, by = c("trial_number", "sequence_scheme", "sequence_scheme_print", "complexity"))
s_sim_b_res <- scaled_s_res_sum %>% 
  left_join(b_res_summary, by = c("trial_number", "sequence_scheme", "sequence_scheme_print", "complexity"))
r_sim_b_res <- scaled_r_res_sum %>% 
  left_join(b_res_summary, by = c("trial_number", "sequence_scheme", "sequence_scheme_print", "complexity"))
nn_sim_b_res <- scaled_nn_res_sum %>% 
  left_join(b_res_summary, by = c("trial_number", "sequence_scheme", "sequence_scheme_print", "complexity"))



df_compare_eig <- calculate_scaled_correlation(eig_sim_b_res) %>% mutate(type = "RANCH (EIG)")
df_compare_kl <- calculate_scaled_correlation(kl_sim_b_res) %>% mutate(type = "RANCH (KL-divergence)")
df_compare_surpirsal <- calculate_scaled_correlation(s_sim_b_res) %>% mutate(type = "RANCH (Surprisal)")
df_compare_random <- calculate_scaled_correlation(r_sim_b_res) %>% mutate(type = "Baseline: No Learning")
df_compare_nn <- calculate_scaled_correlation(nn_sim_b_res) %>% mutate(type = "Baseline: No Noise")

tb_df <- bind_rows(df_compare_eig, df_compare_kl, df_compare_surpirsal, df_compare_nn, df_compare_random) %>% 
  ungroup() %>% 
  mutate(r_in_log = round(r_in_log, 2), 
         rmse_in_log = round(rmse_in_log, 2)) %>% 
  mutate(`Pearson's r` = r_in_log, 
         "RMSE" = rmse_in_log, 
         `Model Type (Linking Hypothesis)` = type) %>% 
  select( `Model Type (Linking Hypothesis)`, `Pearson's r`, RMSE) %>% 
  as.data.frame()

rownames(tb_df) <- NULL

table <- xtable::xtable(as.matrix(tb_df), digits=c(2), 
                       caption = "This table shows the correlations between the log-transformed model results and the log-transformed looking time data. RANCH model implemented with the three different linking hypotheses showed similar performance with slight numerical differences and outperomed the baseline models.")

print.xtable(table, include.rownames = FALSE, comment = FALSE)
```

