---
output: pdf_document
---


To evaluate whether our models can provide sufficient explanation to the behavioral results, we designed a model experiment to represent the behavioral experiments. Then, we searched for the best set of parameters that yielded the highest Pearson’s correlation between the model results and behavioral results. We then compared the model fits within each model’s different linking hypotheses. 


## Model experiment 

To model the behavioral experiment, we first represented the stimuli as a vector of logical values indicating the presence and absence of a feature. All stimuli vectors are length 6, with the complex stimuli represented as having three `TRUE` and simple stimuli represented as having one `TRUE`, The rest of the elements are `FALSE`. Individual stimuli are then assembled into sequences to reflect the stimuli sequences in the behavioral experiment. For a particular sequence, we constructed the deviant stimulus based on the background stimulus to make sure that they were always maximally different and had the same number of features present. 

For Model 1, since it’s behavior is non-probabilistic, we presented the model with each of the four sequences once and derived the information theoretic measurements. For model 2, we ran each sequence 500 times to obtain a reasonably precise estimate on the model’s behaviors. 


```{r include=FALSE}

d_model_res <- readRDS(here("model/report_res/scaled_discrete_model_res.RDS"))
c_model_res <- readRDS(here("model/report_res/scaled_continuous_model_res.RDS"))

```

## Parameter estimation


```{r getparams, include=FALSE}
d_model_params <- d_model_res %>% 
  distinct(params_info) %>% 
  separate(params_info, into = c("ap", "ap_val", 
                                 "bp", "bp_val"), 
           sep = "_")


c_model_params <- c_model_res %>% 
  distinct(im_type, params_info) %>% 
  separate(params_info, into = c("ae", "ae_val", 
                                 "be", "be_val", 
                                 "ap", "ap_val", 
                                 "bp", "bp_val", 
                                 "np", "np_val", 
                                 "wEIG", "wEIG_val"), 
           sep = "_")

d_model_params
c_model_params
```


We performed an iterative grid search in parameter space for each linking hypothesis. We a priori constrained our parameter space on the prior beta distribution to have shape parameters that $\alpha_{\theta} > \beta_{\theta}$, which describe the prior beliefs as “more likely to see the absence of a feature than the presence of a feature”.  For model 1, we searched for the priors over the concept to be learned. The parameter search for the two metrics, surprisal and KL-divergence, converged on the same priors ($\alpha_{\theta}$ = `r d_model_params$ap_val`;$\beta_{\theta}$ = `r d_model_params$bp_val`). For model 2, we searched for the priors over the concept ($\theta$), the noise parameter that decides how likely a feature would be misperceived ($\epsilon$), and the constant EIG from the world ($EIG(world)$). The prior over the noise parameter was fixed for all searches ($\alpha_{\epsilon}$ = 1;$\beta_{\epsilon}$ = 10). In model 2, different parameters were selected to obtain the best fit to the behavioral data (EIG: $\alpha_{\theta}$ = `r filter(c_model_params, im_type == "EIG")$ap_val`, $\beta_{\theta}$ = `r filter(c_model_params, im_type == "EIG")$bp_val`, $\epsilon$ = `r filter(c_model_params, im_type == "EIG")$np_val`, $EIG(world)$ = `r filter(c_model_params, im_type == "EIG")$wEIG_val`; KL: $\alpha_{\theta}$ = `r filter(c_model_params, im_type == "KL")$ap_val`, $\beta_{\theta}$ = `r filter(c_model_params, im_type == "KL")$bp_val`, $\epsilon$ = `r filter(c_model_params, im_type == "KL")$np_val`, $EIG(world)$ = `r filter(c_model_params, im_type == "KL")$wEIG_val`; Surprisal: $\alpha_{\theta}$ = `r filter(c_model_params, im_type == "surprisal")$ap_val`, $\beta_{\theta}$ = `r filter(c_model_params, im_type == "surprisal")$bp_val`, $\epsilon$ = `r filter(c_model_params, im_type == "surprisal")$np_val`, $EIG(world)$ = `r filter(c_model_params, im_type == "surprisal")$wEIG_val`). 



```{r basic_result, echo = FALSE, fig.env = "figure", fig.pos = "h", fig.align='center', fig.width=3, fig.height=3, set.cap.width=T, num.cols.cap=1, fig.cap = "Results from the discrete time model. The top panels show the trajectories of KL under different sequences, and the bottom panels showw the trajectoreis of surprisal."}

d_model_res %>% 
  pivot_longer(cols = c("scaled_log_surprisal", "scaled_log_kl"), 
               names_to = "im_type", 
               values_to = "im_value") %>% 
  mutate(
    im_type_print = case_when(
      im_type == "scaled_log_surprisal" ~ "Surprisal", 
      im_type == "scaled_log_kl" ~ "KL"
    )
  ) %>% 
  ggplot(aes(x = trial_number, y = im_value, color = complexity)) + 
  geom_point() + 
  geom_line() + 
  facet_grid(im_type_print~sequence_scheme_print) + 
  ylab("Scaled Results (log)") + 
  xlab("Trial number") + 
  theme_classic() + 
  langcog::scale_color_solarized(name = "Trial Complexity",labels = c("Complex", "Simple"))  +
  theme(legend.position = "bottom", 
     axis.title = element_text(size = 7.5),
              axis.text.x  = element_text(size = 7.5),
axis.text.y  = element_text(size = 7.5),
        legend.title = element_text(size = 7.5),
        legend.text = element_text(size = 7.5), 
   strip.text.x = element_text(size = 5)

)


```



```{r experiment_res, echo = FALSE, fig.env = "figure*", fig.pos = "h", fig.width=6.6, fig.height=3, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Continuous time model using different linking hypotheses provide qualitatively indistinguishable fits to the behavioral data. All model results are log-transformed and adjusted to be at the same scale and intercepts as the log-transformed behavioral data. The solid lines represent human data, and the dotted lines represent the model's results. Red lines indicated results for complex stimuli, and blue lines indicated results for simple stimuli."}

point_size = 0.3

eig_plot <- c_model_res %>%
  filter(im_type == "EIG") %>%
  rename(res_val = scaled_log_sample_n) %>%
  select(complexity, trial_number, sequence_scheme, sequence_scheme_print, res_val) %>%
  mutate(`Result Type` = "Model") %>%
  bind_rows(
    b_res_print %>% rename(res_val = log_trial_looking_time) %>%
      select(complexity, trial_number, sequence_scheme, sequence_scheme_print, res_val) %>%
      mutate(`Result Type` = "Behavioral")
  ) %>%
   ggplot(aes(x=trial_number, y=res_val, color = complexity, linetype = `Result Type`)) +
   stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2), fatten = point_size) +
   stat_summary(fun.data = "mean_cl_boot",
                geom = "line", position = position_dodge(width = .2)) +

   facet_wrap(~sequence_scheme_print) +
  scale_x_continuous(breaks = seq(1, 6, 1)) +
  #langcog::theme_mikabr() +
  theme_classic()+
  langcog::scale_color_solarized(name = "Trial Complexity",labels = c("Complex", "Simple")) +
  labs(title = "EIG")+
  theme(legend.position = "none",
        axis.title = element_text(size = 8.5),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 6),
        plot.title = element_text(size = 8),
         strip.text.x = element_text(size = 5)
        ) +
  ylab("log Looking time (seconds)") #+
  #xlab("Trial Number")

kl_plot <- c_model_res %>%
  filter(im_type == "KL") %>%
  rename(res_val = scaled_log_sample_n) %>%
  select(complexity, trial_number, sequence_scheme, sequence_scheme_print, res_val) %>%
  mutate(`Result Type` = "Model") %>%
  bind_rows(
    b_res_print %>% rename(res_val = log_trial_looking_time) %>%
      select(complexity, trial_number, sequence_scheme, sequence_scheme_print, res_val) %>%
      mutate(`Result Type` = "Behavioral")
  ) %>%
   ggplot(aes(x=trial_number, y=res_val, color = complexity, linetype = `Result Type`)) +
   stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2), fatten = point_size) +
   stat_summary(fun.data = "mean_cl_boot",
                geom = "line", position = position_dodge(width = .2)) +

   facet_wrap(~sequence_scheme_print) +
  scale_x_continuous(breaks = seq(1, 6, 1)) +
  #langcog::theme_mikabr() +
  theme_classic()+
  langcog::scale_color_solarized(name = "Trial Complexity",labels = c("Complex", "Simple")) +
  labs(title = "KL")+
  theme(legend.position = "none",
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title = element_blank(),
        legend.title = element_text(size = 8.5),
        legend.text = element_text(size = 8.5),
        plot.title = element_text(size = 8),
        strip.text.x = element_text(size = 5)
        )
  #ylab("") +
  #xlab("Trial Number")  +


surprisal_plot <- c_model_res %>%
  filter(im_type == "surprisal") %>%
  rename(res_val = scaled_log_sample_n) %>%
  select(complexity, trial_number, sequence_scheme, sequence_scheme_print, res_val) %>%
  mutate(`Result Type` = "Model") %>%
  bind_rows(
    b_res_print %>% rename(res_val = log_trial_looking_time) %>%
      select(complexity, trial_number, sequence_scheme, sequence_scheme_print, res_val) %>%
      mutate(`Result Type` = "Behavioral")
  ) %>%
   ggplot(aes(x=trial_number, y=res_val, color = complexity, linetype = `Result Type`)) +
   stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2), fatten = point_size) +
   stat_summary(fun.data = "mean_cl_boot",
                geom = "line", position = position_dodge(width = .2)) +

   facet_wrap(~sequence_scheme_print) +
  scale_x_continuous(breaks = seq(1, 6, 1)) +
  #langcog::theme_mikabr() +
  theme_classic()+
  langcog::scale_color_solarized(name = "Trial Complexity",labels = c("Complex", "Simple")) +
  labs(title = "Surpisal")+
  theme(legend.position = "right",
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title = element_blank(),
        plot.title = element_text(size = 8),
         legend.title = element_text(size = 6),
        legend.text = element_text(size =6),
         strip.text.x = element_text(size = 5)
        )
  #xlab("Trial Number")

eig_plot + kl_plot + surprisal_plot
```


## Results and Discussion 

```{r warning=FALSE, include=FALSE}
scaled_eig_res_sum <- summarise_scaled_tidy_sim_res(c_model_res %>% filter(im_type == "EIG"))
scaled_kl_res_sum <- summarise_scaled_tidy_sim_res(c_model_res %>% filter(im_type == "KL"))
scaled_s_res_sum <- summarise_scaled_tidy_sim_res(c_model_res %>% filter(im_type == "surprisal"))
eig_sim_b_res <- scaled_eig_res_sum %>% 
  left_join(b_res_summary, by = c("trial_number", "sequence_scheme", "sequence_scheme_print", "complexity")) 
kl_sim_b_res <- scaled_kl_res_sum %>% 
  left_join(b_res_summary, by = c("trial_number", "sequence_scheme", "sequence_scheme_print", "complexity"))
s_sim_b_res <- scaled_s_res_sum %>% 
  left_join(b_res_summary, by = c("trial_number", "sequence_scheme", "sequence_scheme_print", "complexity"))


df_compare_basic <- calculate_correlation_for_scaled_basic_model(d_model_res)


df_compare_eig <- calculate_scaled_correlation(eig_sim_b_res)
df_compare_kl <- calculate_scaled_correlation(kl_sim_b_res)
df_compare_surpirsal <- calculate_scaled_correlation(s_sim_b_res)
```

Both models reproduced the behavioral phenomena qualitatively, showing habituation, dishabituation, and complexity effect. To quantitatively explore the models, we fit the models’ output to the behavioral data. All models’ results were adjusted to match behavioral data’s scale and intercepts for easier comparisons.   In model 1, we found that KL provided a better fit for the behavioral data than surprisal (Fig X; KL: *r* = `r round(df_compare_basic$kl_r_log, 2)`; RMSE = `r round(df_compare_basic$kl_rmse_log, 2)`; Surprisal: *r* = `r round(df_compare_basic$surprisal_r_log, 2)`; RMSE = `r round(df_compare_basic$surprisal_rmse_log, 2)`) In model 2, however, we found that the three linking hypotheses were qualitatively indistinguishable in their fits (Fig X; EIG: *r* = `r round(df_compare_eig$r_in_log, 2)`, RMSE = `r round(df_compare_eig$rmse_in_log, 2)`; KL: *r* = `r round(df_compare_kl$r_in_log, 2)`, RMSE = `r round(df_compare_kl$rmse_in_log, 2)`; Surprisal: *r* = `r round(df_compare_surpirsal$r_in_log, 2)`, RMSE = `r round(df_compare_surpirsal$rmse_in_log, 2)`). In this section, we discuss the implications of our findings and future directions. 

First, the result from our discrete time model agree with the findings reported by @poli2020infants. We show in a new behavioral dataset that, similar to infants, adults’ looking time is better predicted by KL divergence than surprisal. This converging finding suggests a developmental continuity on principles guiding looking time. Second, the result from our continuous time model showed that under our model architecture, the performance of surprisal and KL can match that of EIG,  a metric that can quantitatively characterize the optimal exploratory behaviors in humans [@oaksford1994rational; @coenen2019asking]. However, to calculate EIG one needs to consider all possible combinations of features for the next observation, which becomes computationally expensive and therefore psychologically implausible quickly. The proximity of model fits between EIG, KL, and surprisal suggests that easier-to-compute metrics can be viable metrics to which to anchor sampling behavior. Our finding contrasts with previous work that tries to dissociate between different metrics using the same behavioral dataset [@poli2020infants; liquin2021developmental]. Last but not least, although we can not directly compare the two models’ fits quantitatively due to the differences in the number of free parameters, our results still show how models’ architectural differences can lead to different conclusions about the linking hypotheses. In a more psychologically realistic modeling regime, different information theoretic measurements can perform similarly.

There are several limitations to the current work. For our behavioral data, one concern is that our self-paced visual presentation task might not be capturing participants’ intrinsic interests in exploring the stimuli, but rather other task demands. However, we found no no differences in looking time across the different attention check conditions. This suggests that the recorded behaviors are independent of task demand, suggesting that  they likely reflect participants’ genuine interests in looking at the stimuli. For our models, there are several limitations that require further investigation. The current stimulus representation is rather oversimplified. For example, we did not take into consideration how features can have different degrees of saliency. In addition, the sampling policy’s implementation can be further challenged. The model currently decides between “continuing looking” and “look away”, but one can argue that in the behavioral experiment the participants were deciding between “continuing looking at the current stimulus” and “look at the next stimulus”. Building these more sophisticated assumptions into the model would certainly help us understand looking time better under a rational analysis framework. Nevertheless, our current work suggests that simpler models are capable of explaining key phenomena in looking time change. 

Our ultimate goal is to provide a computational model that can explain the key looking time patterns documented and utilized in infant research: habituation, dishabituation, and complexity effect. We believe these patterns are driven by information-seeking principles that have strong developmental continuities. In the current work, we have shown that information theoretic measurements can be linked to adult looking time patterns. We also compared the two models and found that model architecture has consequences for linking hypotheses. Specifically, in a discrete time model, we found KL is a better predictor than surprisal. But in the continuous time model, three measurements (EIG, KL, and surprisal) become indistinguishable. As we further elaborate on our modeling approach, our ongoing work on infants will eventually help address the developmental trajectories of the mechanisms through which learners decide what to look at, and when to stop looking.
