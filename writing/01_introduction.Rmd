---
output: pdf_document
---

Whether to keep looking at a current target of attention is one of the most fundamental decisions we make, whether we are trying to find our way in a busy street or swiping through TikTok. Even young infants constantly decide whether to keep looking or move on. In fact, infant research has long capitalized on infants’ ability to endogenously control their attention, making inferences about infants’ learning and mental representations from changes in their looking duration [@aslin2007s; @sim2019another]. In a typical experiment, infants decrease their looking duration upon seeing the same stimulus repeatedly (i.e. habituation). Then, infants’ often recover interest when seeing a novel stimulus (i.e. dishabituation). While these phenomena are well-documented, the mechanisms underlying them remain poorly understood.  A better understanding of what shapes habituation and dishabituation is of both methodological and theoretical significance. Methodologically, assumptions about habituation and dishabituation underpin many other claims about infants' cognitive repertoire [@paulus2022should; @tafreshi2014analysis]. Theoretically, it would shed light on infants’ active role in shaping their own learning and reveal principles that guide human information-seeking behavior in general [@smith2018developing; @raz2020learning].  Here we provide a model of the basic decision faced by infants in a standard looking paradigm. To do so, we model looking as rational active selection of noisy perceptual samples for learning. Our goal is to describe the proximal computations that could underlie the moment-to-moment decision of whether to keep looking at the current stimulus, or look away to find a different stimulus.

Classical theory of infant looking posits that infants look at stimuli in order to learn or encode them, so the dynamics of looking time are driven by the dynamics of learning [@hunter1988multifactor]. The more an infant has already been exposed to a stimulus, the less they have to learn about it (i.e. increasing exposure time should decrease looking). The more complicated a stimulus is, the more that would still remain to learn after any given exposure time (i.e. increasing complexity should increase looking). Individual infants may also differ in how long it takes them to learn a given stimulus (e.g. older infants may habituate after less exposure.  Although this theory is influential, little empirical work has examined it systematically [for exceptions, see @hunter1983effects] and the lack of quantitative details in the theory has made it impossible to offer precise predictions.

<<<<<<< HEAD
In contrast to the classical verbal theory, recent work has attempted to describe infants’ looking behaviors through computational modeling. In pioneering work,  @kidd2012goldilocks developed a paradigm in which infants are shown sequences of events. Infants’ look-away probabilities away from the stimuli are compared with surprisal, a measure of information content, derived from a rational learner model that keeps track of the probabilities of each event. The study shows that infants' pay most attention to event sequences that are neither too high nor too low in surprisal, resulting in a ‘Goldilocks’ effect of attention. A recent study by @poli2020infants offered an alternative linking hypothesis between the model and behavior: the study used a similar paradigm and model to show that infants’ looking time can be predicted by ‘learning progress’, formalized as the Kullback-Leibler (KL) divergence between the the model’s knowledge before and after each stimulus. These attempts to connect information theoretic measures to infants’ looking time resonate with previous literature on information foraging that postulates human exploratory behaviors are driven by maximizing information gain [@hills2008search; @pirolli1999information] as well as the emerging literature on curiosity in developmental robotics and reinforcement learning [@oudeyer2007intrinsic; @haber2018learning]. Curiosity-driven artificial agents’ exploratory behaviors can be guided by optimizing expected information gain (EIG), a measurement that has been shown to be related to curiosity-driven learning in human children and adults [e.g., @liquin2021developmental]. 
=======
In contrast to the classical verbal theory, recent work has attempted to describe infants’ looking behaviors through computational modeling. In pioneering work,  @kidd2012goldilocks developed a paradigm in which infants are shown sequences of events. Infants’ look-away probabilities away from the stimuli are compared with surprisal, a measure of information content, derived from a rational learner model that keeps track of the probabilities of each event. The study shows that infants' pay most attention to event sequences that are neither too high nor too low in surprisal, resulting in a ‘Goldilocks’ effect of attention. A recent study by @poli2020infants offered an alternative linking hypothesis between the model and behavior: the study used a similar paradigm and model to show that infants’ looking time can be predicted by ‘learning progress’, formalized as the Kullback-Leibler (KL) divergence between the the model’s knowledge before and after each stimulus. These attempts to connect information theoretic measures to infants’ looking time resonate with previous literature on information foraging that postulates human exploratory behaviors are driven by maximizing information gain [@hills2015exploration; @pirolli1999information] as well as the emerging literature on curiosity in developmental robotics and reinforcement learning [@oudeyer2007intrinsic; @haber2018learning], as well as information foraging . Curiosity-driven artificial agents’ exploratory behaviors can be guided by optimizing expected information gain (EIG), a measurement that has been shown to be related to curiosity-driven learning in human children and adults [e.g., @liquin2021developmental]. 
>>>>>>> c244988e48d5e3b0e9628716feb18f8fd3b2fdfa

However, there are several limitations to the existing models. First, current models do not capture the noisy nature of perceptual learning [@kersten2004object; @callaway2021fixation]. That is, the models were assumed to acquire perfect representation of each event in the sequence. This assumption leads to the second limitation: While surprisal and KL-divergence have been shown to correlate with infants’ looking behaviors, current models do not provide an account of how these measurements might be linked mechanistically to infants’ behavior. Previous models show that infants might be sensitive to the information-theoretic variability in their learning environment, but they do not provide an account of how they are related to infants’ real-time sampling behavior. Finally, the event sequence paradigm used to evaluate these models are not representative of classical infant looking time paradigms. As the key phenomena described in the Hunter & Ames (1988) theory were not captured, the extent to which we can extrapolate current model fits  to behavior in a typical looking time experiment remains limited.

Here we present steps toward overcoming these limitations. Our goal is to provide a unifying quantitative account of looking behaviors as arising from optimal decision-making over noisy perceptual representations [@callaway2021fixation; @bitzer2014perceptual]. To do so, we present the “rational action, noisy choice for habituation” (RANCH) model. RANCH works by a) accumulating noisy samples from the stimulus, and b) rationally choosing what to look at using different information-theoretic linking hypotheses (surprisal, KL-divergence, and EIG). Finally, we evaluate RANCH with adult looking time data collected from a paradigm that captures habituation, dishabituation, and complexity effects.  

