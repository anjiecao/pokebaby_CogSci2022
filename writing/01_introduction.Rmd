---
output: pdf_document
---

Whether to keep looking at a current target of attention is one of the most fundamental decisions we make, whether we are trying to find our way in a busy street or swiping through TikTok. Even young infants constantly decide  what to look at and for how long. In fact, infant research has capitalized on infants’ ability to endogenously control their attention. Through the use of looking time paradigms, researchers have been able to make inferences about infants’ learning and mental representations from the changes in their looking duration[@aslin2007s; @sim2019another]. In a typical experiment, infants decrease their looking duration upon seeing repeated stimulus (i.e. habituation). After being habituated, infants’ interest will often recover when seeing a novel stimulus, relative to the habituation stimulus (i.e. dishabituation). While these phenomena are well-documented, the mechanisms underlying them remain poorly understood.  A better understanding of what shapes habituation and dishabituation is of both methodological and theoretical significance. Methodologically, it is central to relating behavior to infants’ internal representations. Theoretically, it would shed light on infants’ active role in shaping their own learning [@smith2018developing; raz2020learning] and reveal principles that guide human information-seeking behavior in general.  


Classical theory of infant looking behavior suggests three factors are crucial to habituation and dishabituation: complexity, familiarization time, and infants’ age [@hunter1988multifactor]: 1) Infants will take longer to habituation to complex stimuli, 2) longer familiarization time to one stimulus would make infants more likely to dishabituate to another stimulus and 3) The older the infants, the faster their information processing, and the faster they will habituate. Together, these three factors determine how infants’ looking time changes during an experiment. Although this theory is influential, few empirical work has examined these three factors systematically [for exception, see @hunter1983effects] and the lack of quantitative details in the theory has made it impossible to offer precise predictions.

In contrast to the classical verbal theory, recent work has attempted to describe infants’ looking behaviors through computational modeling. In pioneering work, @kidd2012goldilocks developed a paradigm in which infants are shown sequences of events. Infants’ look-away probabilities toward the stimuli are compared with surprisal, a measure of information content, derived from a rational learner model that keeps track of the probabilities of each event. The study shows that infants' pay most attention to event sequences that are neither too high nor too low in surprisal, resulting in a ‘Golidlocks’ effect of attention. A recent study by @poli2020infants offered an alternative linking hypothesis between the model and behavior: the study used a similar paradigm and model to show that infants’ looking time can be predicted by the Kullback-Leibler (KL) divergence. KL measures the statistical distance between two probabilities distribution, which intuitively tracks ‘learning progress’. These attempts on connecting information theoretic measurements to infants’ looking time resonate with the emerging literature on curiosity in developmental robotics and reinforcement learning [@oudeyer2007intrinsic; @haber2018learning]. Curiosity-driven artificial agents’ exploratory behaviors are guided by optimizing expected information gain (EIG), a measurement that has been shown to be related to curiosity-driven learning iin human children and adults [@liquin2021developmental]. 

However, there are several limitations to the existing models. First, current models do not capture the noisy nature of perceptual learning [@kersten2004object; @callaway2021fixation]. That is, the models were assumed to acquire perfect representation of each event in the sequence. This assumption leads to the second limitation: While surprisal and KL-divergence have been shown to correlate with infants’ looking behaviors, current models do not provide an account of how  these measurements might be linked mechanistically to the infants’ behavior. That is, while these models show that infants might be sensitive to the information-theoretic variability in their learning environment, they do not provide an account of how they are related to infants’ real-time sampling behavior. Finally, the event sequence paradigm used to evaluate these models are not representative of classical infant looking time paradigms. As the key phenomena described in the Hunter & Ames (1988) theory were not captured, the extent to which we can extrapolate current behavior-model fits to  behavior in a typical looking time experiment remains limited.

Here we present steps to overcome these limitations. Our goal is to provide a unifying quantitative account of looking behaviors as arising from optimal decision-making over noisy perceptual representations [@callaway2021fixation; @bitzer2014perceptual]. We begin by instantiating a version of prior learning models in an independent-trial format (where individual stimuli are learned, not sequences of events). We then develop a second model that addresses weakness in previous work by presenting a model that a) accumulates noisy samples from the stimulus, and b) directly chooses what to look at using different information-theoretic linking hypotheses (surprisal, KL-divergence, and EIG). Finally, we evaluate our model with adult looking time data collected from a paradigm that captures habituation, dishabituation, and complexity effects.  

