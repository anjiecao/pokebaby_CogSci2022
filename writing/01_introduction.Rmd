---
output: pdf_document
---

Whether to keep looking at a current target of attention is one of the most fundamental decisions we make, whether we are trying to find our way in a busy street or swiping through TikTok. Even young infants are tasked with making the decision on selecting what to look at and for how long. To look or not to look, this decision that infants make constantly has provided developmental researchers an opportunity to investigate infants’ mental world. Through the use of looking time paradigms, researchers are able to make inferences about infants’ learning and mental representations based on changes in looking time (CITE, CITE, CITE). In a typical experiment, infants increasingly decrease their looking duration upon seeing repeated stimulus (i.e. habituation). When habituated, infants regain their interests when seeing a novel stimulus (i.e. dishabituation). While both phenomena are well-documented, the factors that influence these looking time trajectories remain relatively underexplored.  A better understanding of what shapes habituation and dishabituation is critical given their methodological and theoretical significance. The rise and fall in looking time is not only central to understanding infants’ mental representation, but also shed light on principles that guide information-seeking behavior in general.  

Classical theory of infant looking behavior suggests three factors are crucial to habituation and dishabituation: complexity, familiarization time, and infants’ age (Hunter & Ames, 1988). More perceptually complex stimuli take longer time for infants to habituate. Longer familiarization time to one stimulus would make infants more likely to dishabituate to another stimulus. The older infants are, the more efficient they are at information processing, and the faster they are to habituate when other factors are controlled for. Together, these three factors determine how infants’ looking time changes during an experiment. Although Hunter & Ames (1988) is influential, the evidence for the theory is weak, with some studies showing mixed results (CITE meta analysis). Furthermore, this verbal theory lacks quantitative details, and therefore unlikely to offer precise predictions on looking time changes based on the different factors. 

In contrast to verbal theory, computational models offer quantitative predictions. More recent work has linked infants’ looking behaviors with a range of information theoretic measurements derived from models. In pioneering work, KPA (CITE) developed a paradigm in which infants are shown sequences of events. Infants’ look-away probabilities toward the stimuli are compared with surprisal, a measure of information content, derived from a rational learner model that keeps track of the probabilities of each event. The study shows that infants looking behaviors can be predicted by surprisal. In particular, they pay most attention to event sequences that are neither too high nor too low in surprisal. A recent study with a similar paradigm provides an alternative linking hypothesis. In Poli et al (2020), another information theoretic measurement, Kullback-Leibler divergence, is shown to outperform surprisal in predicting infants’ looking time. These attempts on connecting information theoretic measurements to infants’ looking time resonate with the emerging literature on curiosity in developmental robotics and reinforcement learning (CITE, CITE, CITE). Curiosity-driven artificial agents’  exploratory behaviors are guided by optimizing Expected Information Gain (EIG) (CITE, CITE), a measurement that has been shown to be related to information-seeking in human children and adults as well (CITE). 

However, there are several limitations to the existing models. First, the current models did not capture the noisy nature of perceptual learning (CITE noisy perception?). The rational learner models were assumed to acquire perfect representation of each event in the sequence (CITE model). This assumption leads to the second limitation: the lack of explanation in why a learner would choose to learn a stimulus in the first place. Both surprisal and KL-divergence have been presented as potential explanations of infants’ looking behaviors, yet neither of the measurements is mechanistically linked to the models’ behaviors. They are descriptive in nature, derived from models that track the probabilities of the events. Finally, the behavioral data that the models were evaluated with came from experimental paradigms that were not representatives of infant looking time paradigms. The key phenomena, habituation and dishabituation, were not captured. The extent to which we can extrapolate current behavior-model fits to understand changes in looking time in a typical looking time experiment remains limited. 


Here we present a series of models that can explain patterns in looking time. Our Goal is to provide a unifying quantitative account of looking behaviors as arising from optimal decision-making over noisy perceptual representations (CITE C & G; drif diffusion). We begin by instantiating a version of prior learning models in an independent-trial format (where individual stimuli are learned, not sequences of events). We then develop a second model that addresses weakness in previous work by a) assume the model is accumulating noisy samples from the stimulus, and b) assume the model is choosing what to look at depending on the linking hypotheses (surprisal, KL-divergence, and EIG). FInally, we evaluate our model with adult looking time data collected from a paradigm that captures habituation, dishabituation, and complexity effect.  
