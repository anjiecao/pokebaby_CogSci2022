---
output: pdf_document
---

From trying to find our way through a busy street to swiping through TikTok, people are constantly making the decision of whether to keep looking or to look at something else. Even the youngest infants decide whether to keep looking at what is in front of them or move on [@haith1980rules]. How can we explain this decision-making process? Our goal in the current paper is to provide a model of the basic decision: whether to keep looking at a stimulus. To do so, we model looking as rational active selection of noisy perceptual samples for learning.

Developmental researchers have long capitalized on infants’ ability to control their attention, making inferences about learning and mental representations from changes in looking duration [@baillargeon1985object; @fantz1963pattern]. In a typical experiment, infants decrease their looking duration upon seeing the same stimulus repeatedly (habituation) but recover interest when seeing a novel stimulus (dishabituation) [@colombo2009infant]. While these phenomena are well-documented, the mechanisms underlying them remain poorly understood, even though assumptions about habituation and dishabituation underpin many other claims about infants' cognitive repertoire [@haith1998put; @aslin2007s; @sim2019another]. 

One classic theory of infant looking posits that infants look at stimuli in order to learn from them, so the dynamics of looking time are driven by the dynamics of learning [@hunter1988multifactor]. This theory describes looking duration as a function of exposure, stimulus complexity, and processing difficulty. The more an infant has already been exposed to a stimulus, the less they have yet to learn about it, but the more complicated the stimulus is, the more they have to learn overall, and older infants are assumed to learn faster than younger infants. Although this theory is influential, its predictions are qualitative, not quantitative, and have not been systematically tested (For exception, see Hunter, Ames, & Koopman, 1983; Bergmann & Cristia, 2016). Nevertheless, it is often invoked as a post-hoc explanation of observed patterns in infant data.

Recent work has attempted to overcome these limitations by describing infants’ looking behaviors quantitatively through computational modeling. @kidd2012goldilocks developed a paradigm in which infants are shown sequences of events until they look away. A rational learning model computed the surprisal (negative log probability) of each event. Infants looked away most from events that were either too high or too low in surprisal (a “goldilocks” effect), suggesting that infants might be looking longer at stimuli with an optimal level of information content. 

In this work, surprisal functioned as a quantitative linking hypothesis, connecting between a learning model and data. But other such linking hypotheses are possible. For example, research on information foraging postulates that human exploratory behaviors are driven by maximizing expected information gain [@hills2015exploration; @pirolli1999information]. In this formulation, agents focus on locations or examples where the amount to be learned is on average highest, a conclusion that is ratified by the emerging literature on curiosity in developmental robotics and reinforcement learning [@oudeyer2007intrinsic]. Indeed, EIG provides a good model of curiosity-driven learning in human children and adults [e.g., @liquin2021developmental]. 

Unfortunately, EIG can be difficult to compute. Because EIG is a measure of expected information gain, its computation requires combinatorial search over all future possibilities. As a result, EIG is often approximated by what is termed “learning progress”: the amount of information the agent just learned [@haber2018learning]. Following this intuition, @poli2020infants formalized learning progress as the Kullback-Leibler (KL) divergence between the model’s knowledge before and after each stimulus and found that a higher KL (i.e. more learning progress) predicted longer looking time in a look-away paradigm.

Thus, existing models take important steps towards a quantitative model of infant attention, but they have three key limitations. First, conceptually, these previous models are not models of choice. The models retrospectively fit infants’ overall pattern of attention, without modeling the decision that infants must make in each moment, whether to keep looking at the current stimulus. Second, and relatedly, these models assume that infants acquire a perfect representation of a given stimulus upon exposure. These prior models do not accommodate the noisy nature of perception, and thus cannot explain why infants would have more information to gain from longer looking at the same, already perceived stimulus [@kersten2004object; @callaway2021fixation]. Third, because of the first two limitations, the prior models could only directly predict one specific infant looking behavior given an unusual learning problem: looking away from an ongoing stream of stimuli while learning about event probabilities. Substantial further innovation is required to use these models to generate quantitative predictions for infant looking behavior in more standard habituation-dishabituation experimental designs.

Here we attempt to overcome these limitations by providing a model of looking behaviors as arising from optimal decision-making over noisy perceptual representations [@callaway2021fixation; @bitzer2014perceptual]. We present the rational action, noisy choice for habituation (RANCH) model. RANCH works by accumulating noisy samples and choosing at each moment whether to continue to look at the current stimulus or to look away to the rest of the environment. Critically, RANCH allows us to explore a learning problem closer to the problem faced by infants in a standard habituation experiment: instead of assuming a learner is estimating the probability of events, the model learns a category based on the exemplars that are presented during habituation [@oakes2010using]. Furthermore, the architecture allows us to investigate different information-theoretic linking hypotheses as informing choice, including EIG, surprisal, and KL. We make a preliminary evaluation of the RANCH model using adult looking time data collected from a self-paced habituation paradigm that captures habituation, dishabituation, and how these phenomena are modified by stimulus complexity. We begin by presenting our experiment, since it frames the learning task for our model.
